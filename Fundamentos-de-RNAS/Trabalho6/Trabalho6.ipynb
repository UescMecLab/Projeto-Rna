{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojWiNpkLcNkO"
      },
      "source": [
        "# Trabalho 6\n",
        "**Para o conjunto de dados disponível no arquivo \"Trabalho6dados.xlsx\", utilizar\n",
        "backpropagation por Levenberg-Marquardt para treinar os pesos da RNA criada no Trabalho 5, Parte 2. Testar diferentes condições iniciais e diferentes parâmetros da otimização visando o melhor resultado possível para função custo $\\frac{1}{2} MSE$ Normalizar e desnormalizar os dados.**\n",
        "\n",
        "## **Parte 1:**\n",
        "**Montar uma rede neural artificial cuja saída seja $y$ e as entradas sejam $(x_1, x_2)$ de acordo com:**\n",
        "\n",
        "$$y = \\phi_2 (b_{2,1} + \\sum^2_{i=1} w_{2,1,i}y_i')$$\n",
        "\n",
        "$$y'_i = \\phi_1 (b_{1,i} + \\sum^2_{j=1} w_{1,i,j}x_j)$$\n",
        "\n",
        "- Com $\\phi_1$ tangente hiperbólica e $\\phi_2$ linear.\n",
        "- Os parâmetros $w$ e $b$ são quaisquer.\n",
        "- A rede deve ser montada em um código executável e desenhado seu diagrama de blocos utilizando o diagrama do neurônio artificial e da rede feedforward."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXKL5bax4fhn"
      },
      "source": [
        "#Backpropagation\n",
        "O objetivo da retropropagação do erro é otimizar os pesos para que a rede neural possa aprender a mapear corretamente as entradas para as saídas.\n",
        "\n",
        "Para qualquer problema de aprendizagem supervisionada é necessário encontrar um conjunto de pesos $W$ que minimize a saída de $E(W)$, onde $E(W)$ é a função de perda, ou o erro da rede.\n",
        "\n",
        "Para realizar a retropropagação do erro é necessário utilizar o erro entre o conjunto de dados e a saída da rede neural para calcular o ajuste de parametros ($\\Delta w$) para a otimização dos resultados utilizando Levenberg-Marquardt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qdkmdnc84D3M"
      },
      "source": [
        "Primordialmente, convém definir o erro utilizado na avaliação do processo de treinamento. Dada uma amostra de treinamento:\n",
        "\n",
        "$$  = \\{ x(n), d(n)\\}_{n=1}^N $$\n",
        "\n",
        "O sinal de erro produzido como output do neurônio $j$ é:\n",
        "\n",
        "$$e_j(n) = d_j(n) - y_j(n)$$\n",
        "\n",
        "E portanto, a média quadrática do erro (MSE):\n",
        "\n",
        "$$E(N) = \\frac{1}{2N} \\sum_{n=1}^N \\sum_{j \\in C}  e^2_{j}(n)$$\n",
        "\n",
        "Onde:\n",
        "- $d_j(n)$: n-ésimo elemento do vetor com as saidas desejadas\n",
        "- $y_j(n)$: n-ésimo elemento do vetor com as saidas atuais\n",
        "- $N$: número de amostras\n",
        "- $C$: conjunto de neurônios na camada\n",
        "\n",
        "Dado um neurônio $j$ sendo alimentado por um conjunto de sinais produzidos pela entrada da camada a sua esquerda, temos que o *induced local field*  $v_j(n)$ produzido na entrada da função de ativação associada ao neurônio $j$ é:\n",
        "\n",
        "$$v_j(n) = \\sum_{i=0}^m w_{ji}(n)x_i(n)$$\n",
        "\n",
        "Onde $m$ corresponde a dimensão dos inputs do neurônio $j$. Assim, a saída $y_j(n)$ do neurônio $j$ na n-ésima iteração é dada por:\n",
        "\n",
        "$$y_j(n) = \\phi_j (v_j(n))$$\n",
        "\n",
        "Assim como nos algoritmos de otimização vistos anteriormente, a retropropagação busca aplicar uma correção $\\Delta w_{ji}(n)$ no peso sináptico $w_{ji}(n)$. Essa correção é proporcional a derivada parcial do erro em relação aos pesos $\\frac{\\partial E(n)}{\\partial w_{ji}(n)}$ e pode ser obtida aplicando a regra da cadeia:\n",
        "\n",
        "$$ \\frac{\\partial E(n)}{\\partial w_{ji}(n)} = \\frac{\\partial E(n)}{\\partial e_{j}(n)}\\frac{ \\partial e_j(n)}{\\partial y_{j}(n)}\\frac{ \\partial y_{j}(n)}{\\partial v_{j}(n)}\\frac{\\partial v_{j}(n)}{\\partial w_{ji}(n)} $$\n",
        "\n",
        "Essa derivada parcial representa um fator sensível capaz de determinar a direção de busca dos melhores pesos sinápticos. Seus termos quando expandidos resultam em:\n",
        "\n",
        "$$ \\frac{\\partial E(n)}{\\partial e_{j}(n)} = \\frac{\\partial \\frac{1}{2}e_j^2(n)}{\\partial e_{j}(n)} = e_j(n) $$\n",
        "\n",
        "$$ \\frac{\\partial e_j(n)}{\\partial y_{j}(n)} = \\frac{\\partial  d_j(n) - y_j(n)}{\\partial y_{j}(n)} = -1$$\n",
        "\n",
        "$$ \\frac{\\partial y_j(n)}{\\partial v_{j}(n)} = \\frac{\\partial  \\phi_j (v_j(n))}{\\partial v_{j}(n)} = \\phi_j' (v_j(n)) $$\n",
        "\n",
        "$$ \\frac{\\partial v_j(n)}{\\partial w_{ji}(n)} = \\frac{\\partial w_{ji}(n)x_i(n)}{\\partial w_{ji}(n)} = x_i(n) $$\n",
        "\n",
        "Associando as equações podemos obter o jacobiano para a implementação do algoritmo de levenberg-marquadt:\n",
        "\n",
        "$$ J(W) = \\begin{bmatrix}\n",
        "\\frac{\\partial e_1(n)}{\\partial w_{i1}(n)} & \\cdots  & \\frac{\\partial e_1(n)}{\\partial w_{iN}(n)} \\\\\n",
        " \\vdots & \\ddots  & \\vdots  \\\\\n",
        "\\frac{\\partial e_N(n)}{\\partial w_{i1}(n)} & \\cdots  & \\frac{\\partial e_N(n)}{\\partial w_{iN}(n)} \\\\\n",
        "\\end{bmatrix} $$\n",
        "\n",
        "sendo\n",
        "$$ \\frac{\\partial e_j(n)}{\\partial w^{(o)}_{ij}(n)} = -\\phi'^{(o)}_{j}(v^{(o)}_{j}(n))x^{(o)}_{j}(n)$$\n",
        "\n",
        "para camada de saida, e\n",
        "\n",
        "$$ \\frac{\\partial e_j(n)}{\\partial w^{(o)}_{ij}(n)} = -\\phi'^{(o)}_{j}(v^{(o)}_{j}(n))w^{(o)}_{ij}(n)\\phi'^{(h)}_{j}(v^{(h)}_{j}(n))x^{(h)}_{j}(n) $$\n",
        "\n",
        "para as camadas ocultas.\n",
        "\n",
        "Assim, os parâmetros (e o bias) podem ser atualizados por camada conforme:\n",
        "\n",
        "$$\\Delta W = - (J^T(W).J(W) + \\eta I)^{-1} . J^T(W)E(W)$$\n",
        "$$\\Delta B = - (J^T(B).J(B) + \\eta I)^{-1} . J^T(B)E(B)$$\n",
        "\n",
        "\n",
        "aonde $E(w)$ é o vetor contendo os erros. Não obstante, se fornecermos a uma camada a derivada do erro em relação à sua saída $(∂E/∂Y)$, então ela deve ser capaz de fornecer a derivada do erro em relação à sua entrada $(∂E/∂X)$. É importante destacar que a saída de uma camada é a entrada da próxima camada. O que significa que $∂E/∂X$ para uma camada é $∂E/∂Y$ para a camada anterior. Assim, podemos usar novamente os resultados obtidos a partir da regra da cadeia para obter a entrada da camada oculta\n",
        "\n",
        "$$ \\frac{\\partial E(n)}{\\partial x_{i}(n)} = \\frac{\\partial E(n)}{\\partial e_{j}(n)}\\frac{ \\partial e_j(n)}{\\partial y_{j}(n)}\\frac{ \\partial y_{j}(n)}{\\partial v_{j}(n)}\\frac{\\partial v_{j}(n)}{\\partial x_{i}(n)}  =  -e_j(n)\\phi_j' (v_j(n))w_{ji}(n)$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6KA4RSlcNkc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "class Neuron():\n",
        "    def __init__(self, input_dim, activation_function, index):\n",
        "        self.index = index\n",
        "        self.input_dim = input_dim\n",
        "        self.weights = np.random.rand(input_dim)\n",
        "        self.bias = np.array(np.random.random())\n",
        "        self.activation_function = self.set_activation_function(activation_function)\n",
        "        self.prime_activation = self.set_prime_activation(activation_function)\n",
        "\n",
        "    def summing_junction(self):\n",
        "        return(np.dot(self.weights, self.input) + self.bias)\n",
        "\n",
        "    def process_output(self, input_signal):\n",
        "      self.input = np.array(input_signal)\n",
        "      self.vk = self.summing_junction()\n",
        "      return (self.activation_function(self.vk))\n",
        "\n",
        "    def set_activation_function(self, activation_function):\n",
        "        if (activation_function == 'tanh'):\n",
        "            return lambda x: np.tanh(x)\n",
        "        if (activation_function == 'linear'):\n",
        "            return lambda x: x\n",
        "\n",
        "    def set_prime_activation(self, activation):\n",
        "        if activation == 'tanh':\n",
        "            return lambda x: 1 - np.tanh(x) ** 2\n",
        "        elif activation == 'linear':\n",
        "            return lambda x: 1    \n",
        "    \n",
        "    def set_delta_w(self, output_error, learning_rate, layer_error):\n",
        "      J_w = np.array([(self.prime_activation(self.vk) * self.input.T * layer_error)])\n",
        "      J_ww = np.dot(J_w.T,  J_w)\n",
        "      grad = np.dot(J_w.T, output_error)\n",
        "      return(np.dot(np.linalg.inv(J_ww + np.dot(learning_rate, np.eye(self.input_dim))), grad))\n",
        "\n",
        "    def set_delta_b(self, output_error, learning_rate, layer_error):\n",
        "        J_b = np.array([(self.prime_activation(self.vk) * layer_error[self.index])])\n",
        "        J_bb = np.dot(J_b.T,  J_b)\n",
        "        grad = np.dot(J_b.T, output_error)\n",
        "        return(np.dot(np.linalg.inv(J_bb + np.dot(learning_rate, np.eye(1))), grad))\n",
        "\n",
        "    def set_error_to_propag(self):\n",
        "      return (self.prime_activation(self.vk) * self.weights.T)\n",
        "\n",
        "    def backpropagation(self, output_error, learning_rate, layer_error):\n",
        "        self.delta_w = self.set_delta_w(output_error, learning_rate, layer_error)\n",
        "        self.delta_b = self.set_delta_b(output_error, learning_rate, layer_error)\n",
        "        self.error_to_propag = self.set_error_to_propag()\n",
        "        self.weights -= self.delta_w.flatten()\n",
        "        self.bias -= self.delta_b[0]\n",
        "        return (self.error_to_propag)\n",
        "\n",
        "    def print_backpropagation_parameters(self):\n",
        "        print (\"ΔW.T = \", self.delta_w.T, \" | ΔB = \", self.delta_b.T, \"| φ'(vk).W = \", self.error_to_propag.T)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wI-Dwa4scNkg"
      },
      "outputs": [],
      "source": [
        "class DenseLayer():\n",
        "    def __init__(self):\n",
        "        self.neurons = []\n",
        "\n",
        "    def __init__(self, input_dim, output_dim, activation_function):\n",
        "        self.neurons = [Neuron(input_dim, activation_function, i) for i in range(output_dim)]\n",
        "   \n",
        "    def forward_propagation(self, input_signal):\n",
        "        outputs = []\n",
        "        for neuron in self.neurons:\n",
        "            output = neuron.process_output(input_signal)\n",
        "            outputs.append(output)\n",
        "        return outputs\n",
        "    \n",
        "    def backpropagation(self, output_error, learning_rate, layer_error):\n",
        " #       outputs = []\n",
        "        for neuron in self.neurons:\n",
        "            output = neuron.backpropagation(output_error, learning_rate, layer_error)\n",
        "#            outputs.append(output)\n",
        "        return output\n",
        "\n",
        "    def set_weights(self, weights):\n",
        "        for neuron, weight in zip(self.neurons, weights):\n",
        "            neuron.weights = np.array(weight)\n",
        "\n",
        "    def set_bias(self, bias):\n",
        "        for neuron, b in zip(self.neurons, bias):\n",
        "            neuron.bias = np.array(b)\n",
        "\n",
        "    def get_weights(self):\n",
        "        weights = [neuron.weights for neuron in self.neurons]\n",
        "        return np.vstack(weights)\n",
        "\n",
        "    def get_bias(self):\n",
        "        bias = [neuron.bias for neuron in self.neurons]\n",
        "        return np.vstack(bias)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFR5IK5EcNkh"
      },
      "outputs": [],
      "source": [
        "class Network():\n",
        "    def __init__(self):\n",
        "        self.layers = []\n",
        "        self.forward_outputs = []\n",
        "        self.input_signal = None\n",
        "\n",
        "    def add(self, input_dim, output_dim, activation_function):\n",
        "        new_layer = DenseLayer(input_dim, output_dim, activation_function)\n",
        "        self.layers.append(new_layer)\n",
        "        return (new_layer)\n",
        "\n",
        "    def add_layer(self, layer):\n",
        "        self.layers.append(layer)\n",
        "        return (layer)\n",
        "\n",
        "    def forward_propagation(self, input_signal):\n",
        "        self.input_signal = input_signal\n",
        "        forward_outputs = []\n",
        "        y = input_signal\n",
        "        for layer in self.layers:\n",
        "            y = layer.forward_propagation(y)\n",
        "            forward_outputs.append(y)\n",
        "        self.forward_outputs = np.array(forward_outputs, dtype=type(forward_outputs))\n",
        "        self.y_pred = y\n",
        "\n",
        "    def backpropagation(self, y_desired, learning_rate=0.01):\n",
        "        output_error = np.array([self.y_pred - y_desired])\n",
        "        layer_error = np.array([1])\n",
        "        for layer in reversed(self.layers):\n",
        "            layer_error = layer.backpropagation(output_error, learning_rate, layer_error)\n",
        "\n",
        "    def print_forward_propagation(self):\n",
        "        print (\"--------------------Forward Propagation----------------------\")\n",
        "        tabs = \"\\t\"\n",
        "        print(\"[Input Signal] ---→ \", self.input_signal)\n",
        "        for i, output in enumerate(self.forward_outputs):\n",
        "            layer_name = \"Output Layer\" if i == len(self.forward_outputs) - 1 else f\"Hidden Layer {i}\"\n",
        "            print(tabs, \"|\\n\", tabs, f\"↳[{layer_name}] ---→ \", output)\n",
        "        tabs += \"\\t\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QzM1ofnwZsd"
      },
      "outputs": [],
      "source": [
        "def launchExample():\n",
        "  input_dim = 2\n",
        "  output_dim = 2\n",
        "\n",
        "  # index (i, j, k) == (layer, neuron, input_conection)\n",
        "  input_signal = np.array([2, 4])           # [x1, x2]               --dim--> (1, 2)\n",
        "  hidden_weights = np.array([[0.95, 0.96],  # neuron 11 [w111, w112] --dim--> (1, 2)\n",
        "              [0.8, 0.85]])                 # neuron 12 [w121, w122] --dim--> (1, 2)\n",
        "  hidden_bias = np.array([0.2 , 0.1])       # [b11, b12]             --dim--> (1, 2)  \n",
        "\n",
        "\n",
        "  output_weights = np.array([[0.9, 0.8]])   # neuron 21 [w211, w212] --dim--> (1, 2)\n",
        "  output_bias = np.array([0.3461])          # [b21]                  --dim--> (1, 1)\n",
        "\n",
        "  network = Network()\n",
        "  hidden_layer = network.add(input_dim, output_dim, 'tanh')\n",
        "  output_layer = network.add(output_dim, 1, 'linear')\n",
        "\n",
        "  hidden_layer.set_weights(hidden_weights)\n",
        "  hidden_layer.set_bias(hidden_bias)\n",
        "  output_layer.set_weights(output_weights)\n",
        "  output_layer.set_bias(output_bias)\n",
        "\n",
        "  network.forward_propagation(input_signal)\n",
        "  network.print_forward_propagation()\n",
        "  network.backpropagation(np.array([4]))\n",
        "  network.forward_propagation(input_signal)\n",
        "  network.print_forward_propagation()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 842
        },
        "id": "IZnb8LGbLTu5",
        "outputId": "28a1dc45-5347-43c6-a50b-105767d3988c"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-5e9d7e845b22>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlaunchExample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-3dda94add5df>\u001b[0m in \u001b[0;36mlaunchExample\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m   \u001b[0moutput_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_bias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m   \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_signal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m   \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_forward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m   \u001b[0;31m#network.backpropagation(np.array([4]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-bd49e33618ec>\u001b[0m in \u001b[0;36mforward_propagation\u001b[0;34m(self, input_signal)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_signal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-56faa4547a55>\u001b[0m in \u001b[0;36mforward_propagation\u001b[0;34m(self, input_signal)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mneuron\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneurons\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneuron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_signal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-ccfd9a39cd64>\u001b[0m in \u001b[0;36mprocess_output\u001b[0;34m(self, input_signal)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprocess_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_signal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_signal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msumming_junction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_signal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \"\"\"\n",
            "\u001b[0;32m<ipython-input-7-ccfd9a39cd64>\u001b[0m in \u001b[0;36msumming_junction\u001b[0;34m(self, input_signal)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msumming_junction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_signal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_signal\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprocess_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_signal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: shapes (1,) and (2,1) not aligned: 1 (dim 0) != 2 (dim 0)"
          ]
        }
      ],
      "source": [
        "launchExample()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKC6cRCEcNkk"
      },
      "outputs": [],
      "source": [
        "def launchRandom():\n",
        "  input_dim = 2\n",
        "  output_dim = 2\n",
        "\n",
        "  network = Network()\n",
        "  network.add(input_dim, output_dim, 'tanh')\n",
        "  network.add(output_dim, 1, 'linear')\n",
        "\n",
        "  for _ in range(3):\n",
        "    input_signal = np.random.rand(input_dim)\n",
        "    network.forward_propagation(input_signal)\n",
        "    network.print_forward_propagation()\n",
        "    print()\n",
        "\n",
        "launchRandom()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff48azQy4nU_"
      },
      "source": [
        "# Referências bibliográficas\n",
        "\n",
        "**[1]** HAYKIN, S. **Neural Networks and Learning Machines**. 3ed. Pearson, 2009"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

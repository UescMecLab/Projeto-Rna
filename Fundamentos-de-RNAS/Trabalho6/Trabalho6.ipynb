{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojWiNpkLcNkO"
      },
      "source": [
        "# Trabalho 6\n",
        "**Para o conjunto de dados disponível no arquivo \"Trabalho6dados.xlsx\", utilizar\n",
        "backpropagation por Levenberg-Marquardt para treinar os pesos da RNA criada no Trabalho 5, Parte 2. Testar diferentes condições iniciais e diferentes parâmetros da otimização visando o melhor resultado possível para função custo $\\frac{1}{2} MSE$ Normalizar e desnormalizar os dados.**\n",
        "\n",
        "## **Parte 1:**\n",
        "**Montar uma rede neural artificial cuja saída seja $y$ e as entradas sejam $(x_1, x_2)$ de acordo com:**\n",
        "\n",
        "$$y = \\phi_2 (b_{2,1} + \\sum^2_{i=1} w_{2,1,i}y_i')$$\n",
        "\n",
        "$$y'_i = \\phi_1 (b_{1,i} + \\sum^2_{j=1} w_{1,i,j}x_j)$$\n",
        "\n",
        "- Com $\\phi_1$ tangente hiperbólica e $\\phi_2$ linear.\n",
        "- Os parâmetros $w$ e $b$ são quaisquer.\n",
        "- A rede deve ser montada em um código executável e desenhado seu diagrama de blocos utilizando o diagrama do neurônio artificial e da rede feedforward."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXKL5bax4fhn"
      },
      "source": [
        "#Backpropagation\n",
        "O objetivo da retropropagação do erro é otimizar os pesos para que a rede neural possa aprender a mapear corretamente as entradas para as saídas.\n",
        "\n",
        "Para qualquer problema de aprendizagem supervisionada é necessário encontrar um conjunto de pesos $W$ que minimize a saída de $E(W)$, onde $E(W)$ é a função de perda, ou o erro da rede.\n",
        "\n",
        "Para realizar a retropropagação do erro é necessário utilizar o erro entre o conjunto de dados e a saída da rede neural para calcular o ajuste de parametros ($\\Delta w$) para a otimização dos resultados utilizando Levenberg-Marquardt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qdkmdnc84D3M"
      },
      "source": [
        "Primordialmente, convém definir o erro utilizado na avaliação do processo de treinamento. Dada uma amostra de treinamento:\n",
        "\n",
        "$$  = \\{ x(n), d(n)\\}_{n=1}^N $$\n",
        "\n",
        "O sinal de erro produzido como output do neurônio $j$ é:\n",
        "\n",
        "$$e_j(n) = d_j(n) - y_j(n)$$\n",
        "\n",
        "E portanto, a média quadrática do erro (MSE):\n",
        "\n",
        "$$E(N) = \\frac{1}{2N} \\sum_{n=1}^N \\sum_{j \\in C}  e^2_{j}(n)$$\n",
        "\n",
        "Onde:\n",
        "- $d_j(n)$: n-ésimo elemento do vetor com as saidas desejadas\n",
        "- $y_j(n)$: n-ésimo elemento do vetor com as saidas atuais\n",
        "- $N$: número de amostras\n",
        "- $C$: conjunto de neurônios na camada\n",
        "\n",
        "Dado um neurônio $j$ sendo alimentado por um conjunto de sinais produzidos pela entrada da camada a sua esquerda, temos que o *induced local field*  $v_j(n)$ produzido na entrada da função de ativação associada ao neurônio $j$ é:\n",
        "\n",
        "$$v_j(n) = \\sum_{i=0}^m w_{ji}(n)x_i(n)$$\n",
        "\n",
        "Onde $m$ corresponde a dimensão dos inputs do neurônio $j$. Assim, a saída $y_j(n)$ do neurônio $j$ na n-ésima iteração é dada por:\n",
        "\n",
        "$$y_j(n) = \\phi_j (v_j(n))$$\n",
        "\n",
        "Assim como nos algoritmos de otimização vistos anteriormente, a retropropagação busca aplicar uma correção $\\Delta w_{ji}(n)$ no peso sináptico $w_{ji}(n)$. Essa correção é proporcional a derivada parcial do erro em relação aos pesos $\\frac{\\partial E(n)}{\\partial w_{ji}(n)}$ e pode ser obtida aplicando a regra da cadeia:\n",
        "\n",
        "$$ \\frac{\\partial E(n)}{\\partial w_{ji}(n)} = \\frac{\\partial E(n)}{\\partial e_{j}(n)}\\frac{ \\partial e_j(n)}{\\partial y_{j}(n)}\\frac{ \\partial y_{j}(n)}{\\partial v_{j}(n)}\\frac{\\partial v_{j}(n)}{\\partial w_{ji}(n)} $$\n",
        "\n",
        "Essa derivada parcial representa um fator sensível capaz de determinar a direção de busca dos melhores pesos sinápticos. Seus termos quando expandidos resultam em:\n",
        "\n",
        "$$ \\frac{\\partial E(n)}{\\partial e_{j}(n)} = \\frac{\\partial \\frac{1}{2}e_j^2(n)}{\\partial e_{j}(n)} = e_j(n) $$\n",
        "\n",
        "$$ \\frac{\\partial e_j(n)}{\\partial y_{j}(n)} = \\frac{\\partial  d_j(n) - y_j(n)}{\\partial y_{j}(n)} = -1$$\n",
        "\n",
        "$$ \\frac{\\partial y_j(n)}{\\partial v_{j}(n)} = \\frac{\\partial  \\phi_j (v_j(n))}{\\partial v_{j}(n)} = \\phi_j' (v_j(n)) $$\n",
        "\n",
        "$$ \\frac{\\partial v_j(n)}{\\partial w_{ji}(n)} = \\frac{\\partial w_{ji}(n)x_i(n)}{\\partial w_{ji}(n)} = x_i(n) $$\n",
        "\n",
        "Associando as equações podemos obter o jacobiano para a implementação do algoritmo de levenberg-marquadt:\n",
        "\n",
        "$$ J(W) = \\begin{bmatrix}\n",
        "\\frac{\\partial e_1(n)}{\\partial w_{i1}(n)} & \\cdots  & \\frac{\\partial e_1(n)}{\\partial w_{iN}(n)} \\\\\n",
        " \\vdots & \\ddots  & \\vdots  \\\\\n",
        "\\frac{\\partial e_N(n)}{\\partial w_{i1}(n)} & \\cdots  & \\frac{\\partial e_N(n)}{\\partial w_{iN}(n)} \\\\\n",
        "\\end{bmatrix} $$\n",
        "\n",
        "sendo\n",
        "$$ \\frac{\\partial e_j(n)}{\\partial w^{(o)}_{ij}(n)} = -\\phi'^{(o)}_{j}(v^{(o)}_{j}(n))x^{(o)}_{j}(n)$$\n",
        "\n",
        "para camada de saida, e\n",
        "\n",
        "$$ \\frac{\\partial e_j(n)}{\\partial w^{(o)}_{ij}(n)} = -\\phi'^{(o)}_{j}(v^{(o)}_{j}(n))w^{(o)}_{ij}(n)\\phi'^{(h)}_{j}(v^{(h)}_{j}(n))x^{(h)}_{j}(n) $$\n",
        "\n",
        "para as camadas ocultas.\n",
        "\n",
        "Assim, os parâmetros (e o bias) podem ser atualizados por camada conforme:\n",
        "\n",
        "$$\\Delta W = - (J^T(W).J(W) + \\eta I)^{-1} . J^T(W)E(W)$$\n",
        "$$\\Delta B = - (J^T(B).J(B) + \\eta I)^{-1} . J^T(B)E(B)$$\n",
        "\n",
        "\n",
        "aonde $E(w)$ é o vetor contendo os erros. Não obstante, se fornecermos a uma camada a derivada do erro em relação à sua saída $(∂E/∂Y)$, então ela deve ser capaz de fornecer a derivada do erro em relação à sua entrada $(∂E/∂X)$. É importante destacar que a saída de uma camada é a entrada da próxima camada. O que significa que $∂E/∂X$ para uma camada é $∂E/∂Y$ para a camada anterior. Assim, podemos usar novamente os resultados obtidos a partir da regra da cadeia para obter a entrada da camada oculta\n",
        "\n",
        "$$ \\frac{\\partial E(n)}{\\partial x_{i}(n)} = \\frac{\\partial E(n)}{\\partial e_{j}(n)}\\frac{ \\partial e_j(n)}{\\partial y_{j}(n)}\\frac{ \\partial y_{j}(n)}{\\partial v_{j}(n)}\\frac{\\partial v_{j}(n)}{\\partial x_{i}(n)}  =  -e_j(n)\\phi_j' (v_j(n))w_{ji}(n)$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "u6KA4RSlcNkc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class Neuron():\n",
        "    def __init__(self, input_dim, activation_function, index):\n",
        "        self.index = index\n",
        "        self.input_dim = input_dim\n",
        "        self.weights = np.random.rand(input_dim)\n",
        "        self.bias = np.array(np.random.random())\n",
        "        self.activation_function = self.set_activation_function(activation_function)\n",
        "        self.prime_activation = self.set_prime_activation(activation_function)\n",
        "\n",
        "    def summing_junction(self):\n",
        "        return(np.dot(self.weights, self.input) + self.bias)\n",
        "\n",
        "    def process_output(self, input_signal):\n",
        "      self.input = np.array(input_signal)\n",
        "      self.vk = self.summing_junction()\n",
        "      return (self.activation_function(self.vk))\n",
        "\n",
        "    def set_activation_function(self, activation_function):\n",
        "        if (activation_function == 'tanh'):\n",
        "            return lambda x: np.tanh(x)\n",
        "        if (activation_function == 'linear'):\n",
        "            return lambda x: x\n",
        "\n",
        "    def set_prime_activation(self, activation):\n",
        "        if activation == 'tanh':\n",
        "            return lambda x: 1 - np.tanh(x) ** 2\n",
        "        elif activation == 'linear':\n",
        "            return lambda x: 1    \n",
        "    \n",
        "    def set_delta_w(self, output_error, learning_rate, layer_error):\n",
        "      J_w = np.array([(self.prime_activation(self.vk) * self.input.T * layer_error)])\n",
        "      J_ww = np.dot(J_w.T,  J_w)\n",
        "      grad = np.dot(J_w.T, output_error)\n",
        "      return(np.dot(np.linalg.inv(J_ww + np.dot(learning_rate, np.eye(self.input_dim))), grad))\n",
        "\n",
        "    def set_delta_b(self, output_error, learning_rate, layer_error):\n",
        "        J_b = np.array([(self.prime_activation(self.vk) * layer_error[self.index])])\n",
        "        J_bb = np.dot(J_b.T,  J_b)\n",
        "        grad = np.dot(J_b.T, output_error)\n",
        "        return(np.dot(np.linalg.inv(J_bb + np.dot(learning_rate, np.eye(1))), grad))\n",
        "\n",
        "    def set_error_to_propag(self):\n",
        "      return (self.prime_activation(self.vk) * self.weights.T)\n",
        "\n",
        "    def backpropagation(self, output_error, learning_rate, layer_error):\n",
        "        self.delta_w = self.set_delta_w(output_error, learning_rate, layer_error)\n",
        "        self.delta_b = self.set_delta_b(output_error, learning_rate, layer_error)\n",
        "        self.error_to_propag = self.set_error_to_propag()\n",
        "        self.weights -= self.delta_w.flatten()\n",
        "        self.bias -= self.delta_b[0]\n",
        "        return (self.error_to_propag)\n",
        "\n",
        "    def print_backpropagation_parameters(self):\n",
        "        print (\"ΔW.T = \", self.delta_w.T, \" | ΔB = \", self.delta_b.T, \"| φ'(vk).W = \", self.error_to_propag.T)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "wI-Dwa4scNkg"
      },
      "outputs": [],
      "source": [
        "class DenseLayer():\n",
        "    def __init__(self, index):\n",
        "        self.neurons = []\n",
        "        self.index = index\n",
        "\n",
        "    def __init__(self, input_dim, output_dim, activation_function):\n",
        "        self.neurons = [Neuron(input_dim, activation_function, i) for i in range(output_dim)]\n",
        "   \n",
        "    def forward_propagation(self, input_signal):\n",
        "        outputs = []\n",
        "        for neuron in self.neurons:\n",
        "            output = neuron.process_output(input_signal)\n",
        "            outputs.append(output)\n",
        "        return outputs\n",
        "    \n",
        "    def backpropagation(self, output_error, learning_rate, layer_error):\n",
        "        outputs = []\n",
        "        for neuron in self.neurons:\n",
        "            if (len(layer_error) > 1):\n",
        "                output = neuron.backpropagation(output_error, learning_rate, layer_error[neuron.index])\n",
        "            else:\n",
        "                output = neuron.backpropagation(output_error, learning_rate, layer_error[0])\n",
        "            outputs.append(output)\n",
        "        return np.array(outputs)\n",
        "\n",
        "    def set_weights(self, weights):\n",
        "        for neuron, weight in zip(self.neurons, weights):\n",
        "            neuron.weights = np.array(weight)\n",
        "\n",
        "    def set_bias(self, bias):\n",
        "        for neuron, b in zip(self.neurons, bias):\n",
        "            neuron.bias = np.array(b)\n",
        "\n",
        "    def get_weights(self):\n",
        "        weights = [neuron.weights for neuron in self.neurons]\n",
        "        return np.vstack(weights)\n",
        "\n",
        "    def get_bias(self):\n",
        "        bias = [neuron.bias for neuron in self.neurons]\n",
        "        return np.vstack(bias)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "qFR5IK5EcNkh"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Network():\n",
        "    def __init__(self):\n",
        "        self.layers = []\n",
        "        self.forward_outputs = []\n",
        "        self.input_signal = None\n",
        "\n",
        "    def add(self, input_dim, output_dim, activation_function):\n",
        "        new_layer = DenseLayer(input_dim, output_dim, activation_function)\n",
        "        self.layers.append(new_layer)\n",
        "        return (new_layer)\n",
        "\n",
        "    def forward_propagation(self, input_signal):\n",
        "        self.input_signal = input_signal\n",
        "        forward_outputs = []\n",
        "        y = input_signal\n",
        "        for layer in self.layers:\n",
        "            y = layer.forward_propagation(y)\n",
        "            forward_outputs.append(y)\n",
        "        self.forward_outputs = forward_outputs\n",
        "        self.y_pred = y\n",
        "\n",
        "    def backpropagation(self, y_desired, learning_rate=0.01):\n",
        "        output_error = np.array([self.y_pred - y_desired])\n",
        "        layer_error = np.array([[1]])\n",
        "        for layer in reversed(self.layers):\n",
        "            layer_error = layer.backpropagation(output_error, learning_rate, layer_error)\n",
        "\n",
        "    def print_forward_propagation(self):\n",
        "        print (\"--------------------Forward Propagation----------------------\")\n",
        "        tabs = \"\\t\"\n",
        "        print(\"[Input Signal] ---→ \", self.input_signal)\n",
        "        for i, output in enumerate(self.forward_outputs):\n",
        "            layer_name = \"Output Layer\" if i == len(self.forward_outputs) - 1 else f\"Hidden Layer {i}\"\n",
        "            print(tabs, \"|\\n\", tabs, f\"↳[{layer_name}] ---→ \", output)\n",
        "        tabs += \"\\t\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "2QzM1ofnwZsd"
      },
      "outputs": [],
      "source": [
        "def launchExample():\n",
        "  input_dim = 2\n",
        "  output_dim = 2\n",
        "\n",
        "  # index (i, j, k) == (layer, neuron, input_conection)\n",
        "  input_signal = np.array([2, 4])           # [x1, x2]               --dim--> (1, 2)\n",
        "  hidden_weights = np.array([[0.95, 0.96],  # neuron 11 [w111, w112] --dim--> (1, 2)\n",
        "              [0.8, 0.85]])                 # neuron 12 [w121, w122] --dim--> (1, 2)\n",
        "  hidden_bias = np.array([0.2 , 0.1])       # [b11, b12]             --dim--> (1, 2)  \n",
        "\n",
        "\n",
        "  output_weights = np.array([[0.9, 0.8]])   # neuron 21 [w211, w212] --dim--> (1, 2)\n",
        "  output_bias = np.array([0.3461])          # [b21]                  --dim--> (1, 1)\n",
        "\n",
        "  network = Network()\n",
        "  hidden_layer = network.add(input_dim, output_dim, 'tanh')\n",
        "  output_layer = network.add(output_dim, 1, 'linear')\n",
        "\n",
        "  hidden_layer.set_weights(hidden_weights)\n",
        "  hidden_layer.set_bias(hidden_bias)\n",
        "  output_layer.set_weights(output_weights)\n",
        "  output_layer.set_bias(output_bias)\n",
        "\n",
        "  network.forward_propagation(input_signal)\n",
        "  network.print_forward_propagation()\n",
        "  network.backpropagation(np.array([4]))\n",
        "  network.forward_propagation(input_signal)\n",
        "  network.print_forward_propagation()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 842
        },
        "id": "IZnb8LGbLTu5",
        "outputId": "28a1dc45-5347-43c6-a50b-105767d3988c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------Forward Propagation----------------------\n",
            "[Input Signal] ---→  [2 4]\n",
            "\t |\n",
            " \t ↳[Hidden Layer 0] ---→  [0.9999861449358144, 0.9999256621257941]\n",
            "\t |\n",
            " \t ↳[Output Layer] ---→  [2.046028060142868]\n",
            "--------------------Forward Propagation----------------------\n",
            "[Input Signal] ---→  [2 4]\n",
            "\t |\n",
            " \t ↳[Hidden Layer 0] ---→  [0.9999885118918928, 0.9999726323596576]\n",
            "\t |\n",
            " \t ↳[Output Layer] ---→  [5.924991245629878]\n"
          ]
        }
      ],
      "source": [
        "launchExample()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "aKC6cRCEcNkk"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------Forward Propagation----------------------\n",
            "[Input Signal] ---→  [0.76244364 0.82542809]\n",
            "\t |\n",
            " \t ↳[Hidden Layer 0] ---→  [0.9079212590848692, 0.6459849473479136]\n",
            "\t |\n",
            " \t ↳[Output Layer] ---→  [1.4717069630347615]\n",
            "\n",
            "--------------------Forward Propagation----------------------\n",
            "[Input Signal] ---→  [0.96151023 0.60676564]\n",
            "\t |\n",
            " \t ↳[Hidden Layer 0] ---→  [0.9069303671511225, 0.6050563611439682]\n",
            "\t |\n",
            " \t ↳[Output Layer] ---→  [1.454276871438509]\n",
            "\n",
            "--------------------Forward Propagation----------------------\n",
            "[Input Signal] ---→  [0.01650391 0.7714687 ]\n",
            "\t |\n",
            " \t ↳[Hidden Layer 0] ---→  [0.6587954554670112, 0.5978059358685591]\n",
            "\t |\n",
            " \t ↳[Output Layer] ---→  [1.302565959116442]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def launchRandom():\n",
        "  input_dim = 2\n",
        "  output_dim = 2\n",
        "\n",
        "  network = Network()\n",
        "  network.add(input_dim, output_dim, 'tanh')\n",
        "  network.add(output_dim, 1, 'linear')\n",
        "\n",
        "  for _ in range(3):\n",
        "    input_signal = np.random.rand(input_dim)\n",
        "    network.forward_propagation(input_signal)\n",
        "    network.print_forward_propagation()\n",
        "    print()\n",
        "\n",
        "launchRandom()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff48azQy4nU_"
      },
      "source": [
        "# Referências bibliográficas\n",
        "\n",
        "**[1]** HAYKIN, S. **Neural Networks and Learning Machines**. 3ed. Pearson, 2009"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
